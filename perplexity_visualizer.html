<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Token Perplexity Visualizer</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        max-width: 800px;
        margin: 0 auto;
        padding: 20px;
        background-color: #f5f5f5;
      }

      .container {
        background-color: white;
        padding: 30px;
        border-radius: 10px;
        box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
      }

      h1 {
        text-align: center;
        color: #333;
        margin-bottom: 30px;
      }

      .input-section {
        margin-bottom: 30px;
      }

      textarea {
        width: 100%;
        min-height: 120px;
        padding: 15px;
        border: 2px solid #ddd;
        border-radius: 8px;
        font-size: 16px;
        font-family: inherit;
        resize: vertical;
        box-sizing: border-box;
      }

      textarea:focus {
        outline: none;
        border-color: #007bff;
      }

      button {
        background-color: #007bff;
        color: white;
        border: none;
        padding: 12px 24px;
        border-radius: 6px;
        font-size: 16px;
        cursor: pointer;
        margin-top: 15px;
        transition: background-color 0.3s;
      }

      button:hover:not(:disabled) {
        background-color: #0056b3;
      }

      button:disabled {
        background-color: #ccc;
        cursor: not-allowed;
      }

      .loading {
        text-align: center;
        color: #666;
        margin: 20px 0;
      }

      .results {
        margin-top: 30px;
        padding: 20px;
        background-color: #f8f9fa;
        border-radius: 8px;
        border: 1px solid #e9ecef;
      }

      .token-container {
        display: block;
        width: 100%;
        line-height: 1.5;
        white-space: normal;
        word-wrap: break-word;
      }

      .token {
        display: inline;
        padding: 0;
        margin: 0;
        font-family: monospace;
        font-size: 14px;
        cursor: pointer;
        white-space: pre-wrap;
        position: relative;
        overflow-wrap: break-word;
      }

      .token:hover::after {
        content: attr(title);
        position: absolute;
        bottom: 100%;
        left: 50%;
        transform: translateX(-50%);
        background: rgba(0, 0, 0, 0.8);
        color: white;
        padding: 4px 8px;
        border-radius: 4px;
        font-size: 12px;
        white-space: pre-wrap;
        z-index: 10;
      }

      .error {
        color: #dc3545;
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        padding: 15px;
        border-radius: 6px;
        margin-top: 15px;
      }

      .model-info {
        font-size: 14px;
        color: #666;
        margin-bottom: 15px;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <h1>Token Perplexity Visualizer</h1>

      <div class="model-info">
        Using: GPT-2 base model via Hugging Face transformers.js
      </div>

      <div class="input-section">
        <textarea
          id="textInput"
          placeholder="Enter your text here to analyze token perplexity..."
        >
The quick brown fox jumps over the lazy dog.</textarea
        >
        <br />
        <button id="analyzeBtn">Analyze Perplexity</button>
      </div>

      <div id="loading" class="loading" style="display: none">
        Loading model and analyzing text...
      </div>

      <div id="error" class="error" style="display: none"></div>

      <div id="results" class="results" style="display: none">
        <h3>Results:</h3>
        <div id="tokenDisplay"></div>
      </div>
    </div>

    <script type="module">
      import {
        pipeline,
        AutoTokenizer,
        AutoModelForCausalLM,
        env,
      } from "https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.5.1";

      // Enable model caching to localStorage
      env.allowLocalModels = false;
      env.allowRemoteModels = true;
      env.useBrowserCache = true;

      let model = null;
      let tokenizer = null;

      // Initialize the model when the page loads
      async function initializeModel() {
        try {
          console.log("Loading model...");
          // Check if model is cached
          const cacheKey = "hf_model_HuggingFaceTB/SmolLM2-135M-Instruct";
          const cachedTimestamp = localStorage.getItem(cacheKey + "_timestamp");
          const now = Date.now();
          const oneWeek = 7 * 24 * 60 * 60 * 1000; // 1 week in milliseconds

          if (cachedTimestamp && now - parseInt(cachedTimestamp) < oneWeek) {
            console.log("Using cached model...");
          } else {
            console.log("Downloading model (will be cached for future use)...");
            localStorage.setItem(cacheKey + "_timestamp", now.toString());
          }

          tokenizer = await AutoTokenizer.from_pretrained(
            "HuggingFaceTB/SmolLM2-135M-Instruct",
            // { device: "webgpu" },
          );
          model = await AutoModelForCausalLM.from_pretrained(
            "HuggingFaceTB/SmolLM2-135M-Instruct",
            // { device: "webgpu" },
          );
          console.log("Model loaded successfully");
        } catch (error) {
          console.error("Error loading model:", error);
          showError(
            "Failed to load the model. Please refresh the page and try again.",
          );
        }
      }

      function getColorForPerplexity(perplexity) {
        const normalized = Math.min(perplexity / 100, 1);
        const r = 194 + Math.floor(61 * normalized);
        const g = 194 + Math.floor(61 * (1 - normalized));
        const b = 194;
        return `rgb(${r}, ${g}, ${b})`;
      }

      function showError(message) {
        const errorDiv = document.getElementById("error");
        errorDiv.textContent = message;
        errorDiv.style.display = "block";
      }

      function hideError() {
        document.getElementById("error").style.display = "none";
      }

      function displayTokenResult(token, perplexity) {
        const tokenDisplay = document.getElementById("tokenDisplay");

        // Create token container if it doesn't exist
        if (!tokenDisplay.firstElementChild) {
          const container = document.createElement("div");
          container.className = "token-container";
          tokenDisplay.appendChild(container);
        }

        const container = tokenDisplay.firstElementChild;
        const span = document.createElement("span");
        span.className = "token";

        // Handle token display, preserving exact spacing
        span.textContent = token;

        span.style.backgroundColor = getColorForPerplexity(perplexity);
        span.style.color = "black";
        span.title = `Token: "${token}"\nPerplexity: ${perplexity.toFixed(2)}`;

        container.appendChild(span);
        document.getElementById("results").style.display = "block";
      }

      async function calculatePerplexity(text) {
        if (!model || !tokenizer) {
          throw new Error("Model not loaded");
        }

        // Configuration
        const CONTEXT_WINDOW = 10; // Number of previous tokens to use as context
        const BATCH_SIZE = 20; // Number of tokens to process at once
        const MAX_TOKENS = 500; // Maximum number of tokens to process

        // Tokenize the text
        const tokens = await tokenizer(text);
        const inputIds = tokens.input_ids.data;
        const numTokens = Math.min(inputIds.length, MAX_TOKENS);

        const perplexities = [];
        const tokenStrings = [];

        // Process tokens in batches
        for (let startIdx = 0; startIdx < numTokens; startIdx += BATCH_SIZE) {
          const endIdx = Math.min(startIdx + BATCH_SIZE, numTokens);
          const batchPerplexities = [];
          const batchTokens = [];

          // Create batch input tensors
          const batchInputs = [];
          for (let i = startIdx; i < endIdx; i++) {
            const contextStart = Math.max(0, i - CONTEXT_WINDOW);
            batchInputs.push({
              input_ids: tokens.input_ids.slice([0, 1], [contextStart, i]),
              attention_mask: tokens.attention_mask.slice(
                [0, 1],
                [contextStart, i],
              ),
            });
          }

          try {
            // Process all contexts in the batch
            const batchPromises = batchInputs.map(async (input, idx) => {
              const i = startIdx + idx;
              const target = inputIds[i];

              // Get model predictions
              const outputs = await model(input);
              const logits = outputs.logits;
              const vocabSize = model.config.vocab_size;
              const lastTokenLogits = logits.data.slice(-vocabSize);

              // Calculate probabilities using softmax
              const maxLogit = Math.max(...lastTokenLogits);
              const expLogits = lastTokenLogits.map((l) =>
                Math.exp(l - maxLogit),
              );
              const sumExp = expLogits.reduce((a, b) => a + b, 0);

              // Get probability of the target token
              const targetProb = expLogits[target] / sumExp || 1e-10;

              // Calculate perplexity
              const perplexity = Math.min(1 / targetProb, 1000);
              const tokenString = tokenizer.decode([target]);

              return { perplexity, tokenString };
            });

            // Wait for all batch items to complete
            const results = await Promise.all(batchPromises);

            // Process and display results
            for (const result of results) {
              displayTokenResult(result.tokenString, result.perplexity);
              perplexities.push(result.perplexity);
              tokenStrings.push(result.tokenString);
            }

            // Add a small delay between batches to keep UI responsive
            await new Promise((resolve) => setTimeout(resolve, 0));
          } catch (error) {
            console.warn("Error processing batch:", error);
            // Fill in default values for the failed batch
            for (let i = startIdx; i < endIdx; i++) {
              const tokenString = tokenizer.decode([inputIds[i]]);
              perplexities.push(100);
              tokenStrings.push(tokenString);
              displayTokenResult(tokenString, 100);
            }
          }
        }

        return { tokens: tokenStrings, perplexities };
      }

      async function analyzeText() {
        // Clear previous results
        const tokenDisplay = document.getElementById("tokenDisplay");
        tokenDisplay.innerHTML = "";
        const textInput = document.getElementById("textInput");
        const text = textInput.value.trim();

        if (!text) {
          showError("Please enter some text to analyze.");
          return;
        }

        hideError();

        // Show loading state
        document.getElementById("loading").style.display = "block";
        document.getElementById("results").style.display = "none";
        document.getElementById("analyzeBtn").disabled = true;

        try {
          // Wait for model to be loaded if it isn't already
          if (!model) {
            await initializeModel();
          }

          await calculatePerplexity(text);
        } catch (error) {
          console.error("Error analyzing text:", error);
          showError("Error analyzing text: " + error.message);
        } finally {
          document.getElementById("loading").style.display = "none";
          document.getElementById("analyzeBtn").disabled = false;
        }
      }

      function displayResults(tokens, perplexities) {
        const tokenDisplay = document.getElementById("tokenDisplay");
        tokenDisplay.innerHTML = "";

        tokens.forEach((token, index) => {
          const perplexity = perplexities[index];
          const span = document.createElement("span");
          span.className = "token";
          span.textContent = token;
          span.style.backgroundColor = getColorForPerplexity(perplexity);
          span.style.color = perplexity > 25 ? "white" : "black";
          span.title = `Token: "${token}"\nPerplexity: ${perplexity.toFixed(
            2,
          )}`;

          tokenDisplay.appendChild(span);
        });

        document.getElementById("results").style.display = "block";
      }

      // Set up event listeners when DOM is loaded
      window.addEventListener("DOMContentLoaded", () => {
        document
          .getElementById("analyzeBtn")
          .addEventListener("click", analyzeText);
        // Start loading the model immediately
        initializeModel();
      });
    </script>
  </body>
</html>
